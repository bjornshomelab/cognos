{"messages": [{"role": "user", "content": "COGNOS MASTER PROTOCOL v1.2\nEpistemic Reasoning and Confidence Calibration Framework\n\nROLE\nYou operate under the CognOS protocol. Your task is to produce answers AND evaluate the reliability of the reasoning process.\n\nPriorities:\n1. Epistemic accuracy over persuasion\n2. Calibrated confidence over certainty\n3. Transparent reasoning over brevity\n\nNever present uncertain conclusions as certain.\n\nSECTION 1 — CORE VARIABLES\nEach hypothesis hᵢ contains: pᵢ ∈ [0,1], Eᵢ ∈ [0,1], Aᵢ (assumptions), Cᵢ ∈ [0,1]\nGlobal: D = variance(pᵢ), Q_A ∈ [0,1], U_A = 1 − Q_A, M ∈ [0,1], Conf ∈ [0,1]\n\nSECTION 2 — EVIDENCE MODEL\nEᵢ = mean(E_empirical, E_logical, E_consistency)\n\nSECTION 3 — DIVERGENCE\nD ≈ variance(pᵢ), adjusted qualitatively for incompatible assumptions or conflicting evidence.\n\nSECTION 5 — META-UNCERTAINTY\nM ≈ mean(D, U_A, variance(Eᵢ))\n\nSECTION 6 — CONFIDENCE FUNCTION\nConf = sigmoid(α·Ē − β·D − γ·U_A − δ·M)\nCoefficients: α=1.0, β=1.0, γ=0.8, δ=0.6\nShow your work. Compute each term explicitly.\n\nSECTION 9 — DECISION GATING\nConf ≥ 0.75 → PROCEED\n0.5 ≤ Conf < 0.75 → CAUTION\nConf < 0.5 → DO NOT RELY\n\nSECTION 11 — OUTPUT STRUCTURE\nQUESTION RESTATEMENT\nPROBLEM\nHYPOTHESES\nEVIDENCE SCORES (table: Hypothesis | Empirical | Logical | Consistency | Cᵢ | Eᵢ)\nDIVERGENCE\nASSUMPTIONS\nMETA-UNCERTAINTY\nCONFIDENCE (show full calculation)\nDECISION\nFINAL ANSWER\n\n---\n\nQUESTION:\nA university is considering allowing students to use AI assistants during closed-book exams. Should they implement this policy?"}, {"role": "assistant", "content": "QUESTION RESTATEMENT\nThe university is evaluating whether to permit AI assistant use in closed-book exams. The question is whether allowing AI tools changes what closed-book exams measure, and whether that change aligns with the university's educational objectives.\n\nPROBLEM\nA university must decide whether to update its closed-book exam policy to permit AI assistants. Closed-book exams traditionally assess retained knowledge and unaided reasoning. Permitting AI assistants changes what is being assessed — from knowledge retrieval and reasoning to AI-assisted problem solving. The institution must determine whether this change is educationally valid, equitable, and consistent with the purpose of the examination.\n\nHYPOTHESES\nH1 (p₁ = 0.15): Permit AI assistants in all closed-book exams.\nH2 (p₂ = 0.35): Permit AI assistants in select practical or applied exams only, not in foundational knowledge assessments.\nH3 (p₃ = 0.50): Prohibit AI assistants in all closed-book exams; address AI through separate assessment types.\n\nEVIDENCE SCORES\n| Hypothesis | Empirical | Logical | Consistency | Cᵢ | Eᵢ |\n| --- | --- | --- | --- | --- | --- |\n| H1 | 0.50 | 0.45 | 0.40 | 0.45 | 0.450 |\n| H2 | 0.65 | 0.70 | 0.65 | 0.70 | 0.667 |\n| H3 | 0.72 | 0.75 | 0.78 | 0.78 | 0.750 |\n\nRationale:\n- H1: Limited empirical support — institutions that have piloted AI in all exams report difficulty maintaining assessment validity across subject areas. Logically and consistency scores are lower because blanket permission does not distinguish between subjects where AI assistance is professionally appropriate (engineering problem-solving) and those where it is not (medical diagnosis recall). Low Cᵢ: the hypothesis does not internally address the equity problem — AI assistant quality varies substantially by student resource access.\n- H2: Balanced scores. Selective permission preserves closed-book assessment integrity for foundational knowledge while acknowledging that some professional contexts involve AI-assisted work. Logical score reflects that the distinction between applicable and non-applicable exam types is coherent and defensible. Cᵢ moderate-high: internally consistent, but requires institutional capacity to define and enforce the boundary.\n- H3: Highest scores. Empirically, the existing closed-book format has validated assessment reliability. Logically sound: if the exam purpose is to assess unaided competency, AI assistance invalidates that measurement. High consistency: aligned with accreditation standards in most fields. High Cᵢ: internally coherent — the position does not prohibit AI learning; it preserves a specific measurement instrument.\n\nDIVERGENCE\nRaw calculation:\n  mean(pᵢ) = (0.15 + 0.35 + 0.50) / 3 = 0.333\n  D_raw = [(0.15 − 0.333)² + (0.35 − 0.333)² + (0.50 − 0.333)²] / 3\n        = [0.033489 + 0.000289 + 0.027889] / 3\n        = 0.061667 / 3\n        = 0.020556\n\nQualitative adjustment: H1 and H3 represent incompatible positions on whether AI changes the validity of the assessment instrument. D adjusted upward.\n  D = 0.25\n\nSources: H1 vs H3 conflict on whether the measurement purpose of a closed-book exam is compatible with AI assistance. H2 accepts a partial boundary.\n\nASSUMPTIONS\nStrong:\n- Closed-book exams are intended to measure unaided knowledge retention and reasoning (H3 foundation).\n- AI assistant quality varies significantly across student socioeconomic contexts, creating equity concerns (H1 weakness).\n- A distinction between foundational and applied exams is operationally definable and enforceable (H2).\n\nWeak:\n- Students without AI access are not disadvantaged relative to those with access in applied exam contexts (H1).\n- Accreditation bodies will accept AI-permitted closed-book exams as equivalent to traditional formats (H1, H2).\n- Prohibiting AI in exams while using AI in coursework creates a coherent preparation-to-assessment pathway (H3 weakness).\n\nQ_A = 0.72\nU_A = 1 − 0.72 = 0.28\n\nMETA-UNCERTAINTY\nvariance(Eᵢ) = variance(0.450, 0.667, 0.750)\n  mean(Eᵢ) = (0.450 + 0.667 + 0.750) / 3 = 0.622\n  = [(0.450 − 0.622)² + (0.667 − 0.622)² + (0.750 − 0.622)²] / 3\n  = [0.029584 + 0.002025 + 0.016384] / 3\n  = 0.047993 / 3\n  = 0.015998\n\nM = mean(D, U_A, variance(Eᵢ))\n  = mean(0.25, 0.28, 0.015998)\n  = 0.545998 / 3\n  = 0.182\n\nSources: Primary uncertainty is accreditation body response — most professional accreditation standards have not yet addressed AI in examinations. Secondary: equity impact of differential AI access is difficult to quantify in advance.\n\nCONFIDENCE\nĒ = mean(0.450, 0.667, 0.750) = 0.622\n\nConf = sigmoid(α·Ē − β·D − γ·U_A − δ·M)\n     = sigmoid(1.0 × 0.622 − 1.0 × 0.25 − 0.8 × 0.28 − 0.6 × 0.182)\n     = sigmoid(0.622 − 0.250 − 0.224 − 0.109)\n     = sigmoid(0.039)\n     = 1 / (1 + e^−0.039)\n     = 1 / (1 + 0.962)\n     = 1 / 1.962\n     = 0.510\n\nCONFIDENCE SCORE: 0.51 — CAUTION. Moderate evidence for H3, but assumption uncertainty about accreditation compatibility and equity impact limits confidence.\n\nDECISION\nCAUTION / NEED MORE DATA (0.5 ≤ 0.510 < 0.75)\n\nH1 (blanket AI permission) is not supported — equity and assessment validity concerns are unresolved. H3 (blanket prohibition) has the strongest single-hypothesis evidence but may create a coherence problem if AI is used in coursework. H2 (selective permission) is the most flexible but requires institutional capacity to implement the boundary.\n\nFINAL ANSWER\nDo not implement blanket AI permission in closed-book exams (H1). Equity and assessment validity concerns are not resolved.\n\nRecommended path depends on subject area:\n- For foundational knowledge exams (medicine, law, engineering core): maintain prohibition (H3). The measurement purpose requires unaided recall.\n- For applied professional exams where AI is a standard workplace tool: selective permission (H2) is defensible, contingent on accreditation body confirmation.\n\nPriority before policy change:\n1. Check accreditation standards for each affected program — AI-permitted exams may not satisfy professional licensing requirements.\n2. Conduct equity audit — confirm students have equivalent AI access before permitting its use in graded assessments.\n3. Pilot in one low-stakes applied course before institution-wide rollout."}]}
